{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alper/Tools/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking,Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "import codecs\n",
    "import random\n",
    "import locale\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, 'tr_TR.utf8')\n",
    "lower_map = {\n",
    "    ord(u'I'): u'ı',\n",
    "    ord(u'İ'): u'i',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPoemText():\n",
    "    with codecs.open('kucukiskender.txt', \"r\", \"UTF-8\") as f:\n",
    "        text = f.read().replace('\\r\\n','\\n').replace('\\n\\n', '\\n')\n",
    "        firstLines = [k[0] if k[0] else k[1] for k in [e.split('\\n') for e in text.split('***')]]\n",
    "        text=text.replace(':','').replace('\\t','').replace('~','').replace('â','').replace('***', '').replace('1','').replace('2','').replace('3','').replace('4','').replace('5','').replace('6','').replace('7','').replace('8','').replace('9','').replace('0','').replace('-','').replace('\\x91', '').replace('\\x92', '').replace('\\x93','').replace('*','').replace('\\x94','').replace('(','').replace(')','').replace('_','').replace('&','').replace('^','').replace('/', '').replace(\"'\", \"\")\n",
    "        text = text.translate(lower_map).lower()\n",
    "        #words = word_tokenize(text)\n",
    "\n",
    "        return firstLines, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstLines, text=readPoemText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"küçük chopin'e\", 'Ne idüğü belirsiz kelimeler takip ediyor beni! ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstLines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 39\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(','.join(text)))\n",
    "print('Total chars: %s' % len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, ',': 3, '.': 4, ';': 5, '?': 6, 'a': 7, 'b': 8, 'c': 9, 'd': 10, 'e': 11, 'f': 12, 'g': 13, 'h': 14, 'i': 15, 'j': 16, 'k': 17, 'l': 18, 'm': 19, 'n': 20, 'o': 21, 'p': 22, 'q': 23, 'r': 24, 's': 25, 't': 26, 'u': 27, 'v': 28, 'w': 29, 'x': 30, 'y': 31, 'z': 32, 'ç': 33, 'ö': 34, 'ü': 35, 'ğ': 36, 'ı': 37, 'ş': 38}\n"
     ]
    }
   ],
   "source": [
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 120\n",
    "step = 1\n",
    "seq_in = []\n",
    "seq_out = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(text) - maxlen, step):\n",
    "    seq_in.append(text[i: i + maxlen])\n",
    "    seq_out.append(text[i + maxlen])\n",
    "\n",
    "X = np.zeros((len(seq_in), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(seq_in), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(seq_in):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[seq_out[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./weights/{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(maxlen, input_shape=(maxlen, len(chars)), return_sequences=True), input_shape=(maxlen, len(chars))))\n",
    "model.add(Bidirectional(GRU(len(chars), return_sequences=False)))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/5\n",
      "242720/242720 [==============================] - 729s 3ms/step - loss: 2.3144\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.31436, saving model to ./weights/01-2.3144-bigger.hdf5\n",
      "Epoch 2/5\n",
      "242720/242720 [==============================] - 769s 3ms/step - loss: 2.0464\n",
      "\n",
      "Epoch 00002: loss improved from 2.31436 to 2.04636, saving model to ./weights/02-2.0464-bigger.hdf5\n",
      "Epoch 3/5\n",
      "242720/242720 [==============================] - 778s 3ms/step - loss: 1.9438\n",
      "\n",
      "Epoch 00003: loss improved from 2.04636 to 1.94377, saving model to ./weights/03-1.9438-bigger.hdf5\n",
      "Epoch 4/5\n",
      "242720/242720 [==============================] - 812s 3ms/step - loss: 1.8722\n",
      "\n",
      "Epoch 00004: loss improved from 1.94377 to 1.87222, saving model to ./weights/04-1.8722-bigger.hdf5\n",
      "Epoch 5/5\n",
      "242720/242720 [==============================] - 817s 3ms/step - loss: 1.8167\n",
      "\n",
      "Epoch 00005: loss improved from 1.87222 to 1.81675, saving model to ./weights/05-1.8167-bigger.hdf5\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed:\n",
      " \"Köpeğin havladığı spiral\n",
      "\"\n",
      "\n",
      "Köpeğin havladığı spiral\n",
      " teni bir bir alına beni senin aradığı olmadım çocuğu\n",
      "bir sevgilim çıkar benim anlıyor sevişmesinde bir meyanbahar biz beni karantilar kalmış sonr\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed:\n",
      " \"Köpeğin havladığı spiral\n",
      "\"\n",
      "\n",
      "Köpeğin havladığı spiral\n",
      "acak bir insanın!  bucuman tavdım de yok bakır\n",
      "motorkine yasını hiya da bir kapış gömme dai\n",
      "herkeli son izinden yak olmayı, sözlekler aslım başın \n",
      "\n",
      "----- diversity: 1.5\n",
      "----- Generating with seed:\n",
      " \"Köpeğin havladığı spiral\n",
      "\"\n",
      "\n",
      "Köpeğin havladığı spiral\n",
      "h ökter\n",
      "ben...\n",
      "çekildi, d\n",
      "luh içen iççeki ülir! cum üzü yarvatl ışkanlığı soraklar\n",
      "birviliği bir yerkesinlen toğluduğum,son\n",
      "çıkoc\n",
      "kemse gizverde\n",
      "h\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Epoch 1/5\n",
      "242720/242720 [==============================] - 779s 3ms/step - loss: 1.7709\n",
      "\n",
      "Epoch 00001: loss improved from 1.81675 to 1.77091, saving model to ./weights/01-1.7709-bigger.hdf5\n",
      "Epoch 2/5\n",
      "104192/242720 [===========>..................] - ETA: 7:26 - loss: 1.7369"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 1000):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration*5)\n",
    "    seed = firstLines[random.randint(0, len(firstLines)-1)]+'\\r\\n'\n",
    "    model.fit(X, y, batch_size=128, epochs=5, callbacks=callbacks_list)\n",
    "    for diversity in [0.5, 1.0, 1.5]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "            generated = ''\n",
    "            generated += seed\n",
    "            print('----- Generating with seed:\\n \"' + seed + '\"\\n')\n",
    "            sys.stdout.write(generated)\n",
    "                \n",
    "            for i in range(len(generated)+maxlen):\n",
    "                x = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
